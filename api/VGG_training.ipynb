{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hotdog_VGG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ipLIV3ct8tv"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/krisograbek/hotdog/blob/main/api/VGG_training.ipynb)\n",
        "<p align=\"center\">\n",
        "    <a href=\"https://colab.research.google.com/github/krisograbek/hotdog/blob/main/api/VGG_training.ipynb\">\n",
        "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "    </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPcxc_zuNczO"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The purpose of this notebook is to train an image classification model for the Hot Dog Not Hot Dog dataset. The notebook consists of the following steps:\n",
        "\n",
        " - Using kaggle API for downloading\n",
        " - Applying Data Augmentation with `ImageDataGenerator`\n",
        " - Applying Transfer Learning for the VGG19 model\n",
        " - Restoring a model with the lowest validation loss\n",
        " - Applying Early stopping [optional]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXwkrg2lK3XB"
      },
      "source": [
        "## Apply kaggle API\n",
        "\n",
        "We'll use kaggle API in order to download the Hot Dog - Not Hot Dog dataset. [Link to dataset](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog). We chose to use kaggle because this is a reliable source. Usually, many links for downloading datasets are outdated. On kaggle, datasets stay for years. \n",
        "\n",
        "In order to use kaggle API you need a kaggle account with a valid API key. All the steps are explained under [Easiest way to download kaggle data in Google Colab](https://www.kaggle.com/general/74235)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ZhgyV5XPQD"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbvclRuuSQ6C"
      },
      "source": [
        "Please uncomment the following cell. I commented it because my API Key would be visible in the output of the cell. I wanted to hide it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SdIAmzWz51t"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNjWEfi1w-q",
        "outputId": "5b6ae435-d539-4424-b78f-0629fde37b6f"
      },
      "source": [
        "# create a dir for kaggle API\n",
        "! mkdir ~/.kaggle\n",
        "# copy the json file with API Key\n",
        "! cp kaggle.json ~/.kaggle\n",
        "# make it read-only\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# download dataset and unzip it locally\n",
        "! kaggle datasets download -d dansbecker/hot-dog-not-hot-dog -p /content/sample_data/ --unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading hot-dog-not-hot-dog.zip to /content/sample_data\n",
            " 92% 82.0M/89.3M [00:00<00:00, 68.3MB/s]\n",
            "100% 89.3M/89.3M [00:00<00:00, 95.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdzOHLqhWCuK"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1WDqtECRxts"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Vcm3xY2UBN"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aW0F_xbR0FU"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "The dataset is relatively small. It contains only 498 images in the train set. Applying Data Augmentation is a very convienient way to artifficially increase the number of train images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MQerYmp61r-"
      },
      "source": [
        "# Instantiate two image generator classes:\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    data_format='channels_last',\n",
        "    rotation_range=30,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='reflect')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    data_format='channels_last')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW85XJ5N6330",
        "outputId": "f6e3b8da-0665-4e1b-ff7d-ff56bf79af61"
      },
      "source": [
        "# Define the batch size:\n",
        "batch_size=32\n",
        "\n",
        "# Define the train and validation generators: \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory='/content/sample_data/train',\n",
        "    target_size=(224, 224),\n",
        "    classes=['hot_dog','not_hot_dog'],\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=42)\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    directory='/content/sample_data/test',\n",
        "    target_size=(224, 224),\n",
        "    classes=['hot_dog','not_hot_dog'],\n",
        "    class_mode='categorical',\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 498 images belonging to 2 classes.\n",
            "Found 500 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pg6f18zV937"
      },
      "source": [
        "## Downloading VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7by7bV16Ieb"
      },
      "source": [
        "vgg19 = VGG19(include_top=False,\n",
        "              weights='imagenet',\n",
        "              input_shape=(224,224,3),\n",
        "              pooling=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRvdjwiWWK5U"
      },
      "source": [
        "### Freezing layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M9nbUDG6ut4"
      },
      "source": [
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjzG19EPWN7O"
      },
      "source": [
        "## Implementing a Keras Sequential Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OC_qgIf6xCF"
      },
      "source": [
        "# Instantiate the sequential model and add the VGG19 model: \n",
        "model = Sequential()\n",
        "model.add(vgg19)\n",
        "\n",
        "# Add the custom layers atop the VGG19 model: \n",
        "model.add(Flatten(name='flattened'))\n",
        "model.add(Dropout(0.5, name='dropout'))\n",
        "model.add(Dense(2, activation='softmax', name='predictions'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqiJM8WuWWtU"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7viq65fb7Jgg",
        "outputId": "712e8b10-f4cc-4886-bb1e-d2f0a7426cf6"
      },
      "source": [
        "# apply early stoping if no improvement for at least 5 steps (patience)\n",
        "# set restore to True\n",
        "early_stoping_callback=keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', min_delta=0, patience=5, verbose=2, mode='auto',\n",
        "    baseline=None, restore_best_weights=True)\n",
        "\n",
        "checkpoint_filepath = '/content/sample_data/weights.{epoch:02d}.h5'\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss', #  validation loss\n",
        "    mode='min',         # lowest possible\n",
        "    save_best_only=True)\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train model\n",
        "history1 = model.fit(train_generator, steps_per_epoch=15, \n",
        "                    epochs=15, validation_data=valid_generator, \n",
        "                    validation_steps=15, \n",
        "                    callbacks=[\n",
        "                            #    early_stoping_callback, # early stopping \n",
        "                               model_checkpoint_callback # only best model \n",
        "                               ]\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "15/15 [==============================] - 14s 888ms/step - loss: 0.8617 - accuracy: 0.5987 - val_loss: 0.8252 - val_accuracy: 0.5896\n",
            "Epoch 2/15\n",
            "15/15 [==============================] - 13s 859ms/step - loss: 0.6995 - accuracy: 0.6760 - val_loss: 0.4640 - val_accuracy: 0.7729\n",
            "Epoch 3/15\n",
            "15/15 [==============================] - 13s 865ms/step - loss: 0.4497 - accuracy: 0.7940 - val_loss: 0.4579 - val_accuracy: 0.7812\n",
            "Epoch 4/15\n",
            "15/15 [==============================] - 13s 864ms/step - loss: 0.4103 - accuracy: 0.8090 - val_loss: 0.6384 - val_accuracy: 0.7021\n",
            "Epoch 5/15\n",
            "15/15 [==============================] - 13s 854ms/step - loss: 0.3905 - accuracy: 0.8262 - val_loss: 0.6002 - val_accuracy: 0.7292\n",
            "Epoch 6/15\n",
            "15/15 [==============================] - 13s 855ms/step - loss: 0.3651 - accuracy: 0.8369 - val_loss: 0.4716 - val_accuracy: 0.7750\n",
            "Epoch 7/15\n",
            "15/15 [==============================] - 13s 849ms/step - loss: 0.3429 - accuracy: 0.8455 - val_loss: 0.4569 - val_accuracy: 0.7875\n",
            "Epoch 8/15\n",
            "15/15 [==============================] - 13s 855ms/step - loss: 0.2699 - accuracy: 0.8906 - val_loss: 0.4655 - val_accuracy: 0.7771\n",
            "Epoch 9/15\n",
            "15/15 [==============================] - 13s 848ms/step - loss: 0.3025 - accuracy: 0.8498 - val_loss: 0.4626 - val_accuracy: 0.7792\n",
            "Epoch 10/15\n",
            "15/15 [==============================] - 13s 851ms/step - loss: 0.3167 - accuracy: 0.8691 - val_loss: 0.5782 - val_accuracy: 0.7667\n",
            "Epoch 11/15\n",
            "15/15 [==============================] - 13s 849ms/step - loss: 0.2809 - accuracy: 0.8970 - val_loss: 0.4729 - val_accuracy: 0.7792\n",
            "Epoch 12/15\n",
            "15/15 [==============================] - 13s 850ms/step - loss: 0.2329 - accuracy: 0.8991 - val_loss: 0.5305 - val_accuracy: 0.7792\n",
            "Epoch 13/15\n",
            "15/15 [==============================] - 13s 850ms/step - loss: 0.2287 - accuracy: 0.9120 - val_loss: 0.5182 - val_accuracy: 0.7771\n",
            "Epoch 14/15\n",
            "15/15 [==============================] - 13s 855ms/step - loss: 0.2208 - accuracy: 0.8927 - val_loss: 0.5248 - val_accuracy: 0.7667\n",
            "Epoch 15/15\n",
            "15/15 [==============================] - 13s 856ms/step - loss: 0.2674 - accuracy: 0.8798 - val_loss: 0.6308 - val_accuracy: 0.7625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiJ1-0AmWarJ"
      },
      "source": [
        "## Evaluating the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD-OG19xQWAe"
      },
      "source": [
        "### Model after the last epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol_Id5ilF2UG",
        "outputId": "1b4b4fdd-a372-4330-ac51-e2e62a3050f2"
      },
      "source": [
        "score = model.evaluate(valid_generator, verbose=0)\n",
        "print(f\"Test loss: {score[0]}\")\n",
        "print(f\"Test accuracy: {score[1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.626677930355072\n",
            "Test accuracy: 0.7599999904632568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp6OVbnCQcNL"
      },
      "source": [
        "### The best Model\n",
        "However, the 7th epoch had the lowest validation loss. Let's load the model and compare both models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9O3KU6kOM2r",
        "outputId": "9669aa90-f6c9-49fb-f4da-6e955034ed90"
      },
      "source": [
        "best_model = keras.models.load_model('/content/sample_data/weights.07.h5')\n",
        "best_score = best_model.evaluate(valid_generator, verbose=0)\n",
        "print(f\"Test loss: {best_score[0]}\")\n",
        "print(f\"Test accuracy: {best_score[1]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.455517053604126\n",
            "Test accuracy: 0.7879999876022339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oE0DAmnQrOx"
      },
      "source": [
        "The model that we had after 7th epoch performs better. This is our baseline model for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuVhzCWDTRlW"
      },
      "source": [
        "# Final Thoughts\n",
        "\n",
        "In this notebook we showed how to apply transfer learning using a VGG19 model. We achieved around 80% accuracy. It may not seem impressive. Yet, the Hot Dog Dataset is quite small. There are many images that are tricky. If an image contains only a hot dog, our model classifies it correctly with a high probability. Probably, the model could increase accuracy by applying Fine-tuning. It is out of the scope of this notebook.\n",
        "\n",
        "These steps probably won't bring any value:\n",
        "\n",
        "- we could increase the number of epochs. However, it's obvious the model starts to overfit. The difference between train and validation metrics increases.\n",
        "- applying early stopping when there are only 15 epochs isn't beneficial.\n",
        "\n",
        "Further exploration:\n",
        "\n",
        "- compare performance without Data Augmentation\n",
        "- apply Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMTvZEHQ8p5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}